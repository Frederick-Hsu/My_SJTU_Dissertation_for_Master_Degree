% !TEX root = ../main.tex

\chapter{实验的部分源代码}\label{chap:reproduce_experiment}

以下为实验中编写的部分源代码，以供复现实验结果或者帮助理解论文中的一些关键的计算过程与算法。

为复现注意力蒸馏后的效果，请查看以下一些代码片段:
\begin{codeblock}[language=python]

import numpy as np
import SimpleITK as sitk
from matplotlib import pyplot as plt

def load_CT_scan_3D_image(niigz_file_name):
    itkimage = sitk.ReadImage(niigz_file_name)
    numpyImages = sitk.GetArrayFromImage(itkimage)
    numpyOrigin = np.array(list(reversed(itkimage.GetOrigin())))
    numpySpacing = np.array(list(reversed(itkimage.GetSpacing())))
    return numpyImages, numpyOrigin, numpySpacing

#================================================================================

ATM_074_0000_niigz_files = ["ATM_074_0000-feat6.nii.gz", 
                            "ATM_074_0000-feat7.nii.gz", 
                            "ATM_074_0000-feat8.nii.gz", 
                            "ATM_074_0000-feat9.nii.gz"]

feat6_image_npy, feat6_origin, feat6_spacing = 
	load_CT_scan_3D_image(ATM_074_0000_niigz_files[0])

feat7_image_npy, feat7_origin, feat7_spacing = 
	load_CT_scan_3D_image(ATM_074_0000_niigz_files[1])

feat8_image_npy, feat8_origin, feat8_spacing = 
	load_CT_scan_3D_image(ATM_074_0000_niigz_files[2])

feat9_image_npy, feat9_origin, feat9_spacing = 
	load_CT_scan_3D_image(ATM_074_0000_niigz_files[3])
	

depth, height, width = feat6_image_npy.shape

plt.figure(figsize=(10, 10))
plt.imshow(np.flipud(feat6_image_npy[:, height//2, :]))
plt.imshow(np.flipud(feat6_image_npy[:, height//2, :]), cmap="gray")

plt.figure(figsize=(10, 10))
plt.imshow(np.flipud(feat7_image_npy[:, height//2, :]))
plt.imshow(np.flipud(feat7_image_npy[:, height//2, :]), cmap="gray")

plt.figure(figsize=(10, 10))
plt.imshow(np.flipud(feat8_image_npy[:, height//2, :]))
plt.imshow(np.flipud(feat8_image_npy[:, height//2, :]), cmap="gray")

plt.figure(figsize=(10, 10))
plt.imshow(np.flipud(feat9_image_npy[:, height//2, :]))
plt.imshow(np.flipud(feat9_image_npy[:, height//2, :]), cmap="gray")
\end{codeblock}


通道级特征再学习方法的实现：
\begin{codeblock}[language=Python]

import torch.nn as nn

class FeatureRecalibrationModule(nn.Module):
    def __init__(self, num_channels, Depth, Height, Width, reduction_ratio=2):
        super().__init__()
        num_reduced_channels = num_channels // reduction_ratio
        self.reduction_ratio = reduction_ratio
        self.conv_module = nn.Sequential(
            nn.Conv3d(in_channels=num_channels, 
                      out_channels=num_reduced_channels, 
                      kernel_size=1, 
                      stride=1),
            nn.ReLU(inplace=True),
            nn.Conv3d(in_channels=num_reduced_channels, 
                      out_channels=num_channels, 
                      kernel_size=1, 
                      stride=1),
            nn.Sigmoid())
        self.spatial_dimension = [Depth, Height, Width]
        self.Depth_squeeze  = nn.Conv3d(in_channels=Depth,  
                                        out_channels=1, 
                                        kernel_size=1, 
                                        stride=1)
        self.Height_squeeze = nn.Conv3d(in_channels=Height, 
                                        out_channels=1, 
                                        kernel_size=1, 
                                        stride=1)
        self.Width_squeeze  = nn.Conv3d(in_channels=Width,  
                                        out_channels=1, 
                                        kernel_size=1, 
                                        stride=1)
    def forward(self, input_tensor):
        squared_tensor = torch.pow(input_tensor, exponent=2)
        # Weight along channels and different axes
        Depth, Height, Width = self.spatial_dimension[0], \
                               self.spatial_dimension[1], \
                               self.spatial_dimension[2]
        Depth_axis = input_tensor.permute(0, 2, 1, 3, 4)        # B, D, C, H, W
        Height_axis = input_tensor.permute(0, 3, 2, 1, 4)       # B, H, D, C, W
        Z_spatial_integration_on_Depth = \
            self.Height_squeeze(Height_axis).permute(0, 4, 2, 1, 3)
        Z_spatial_integration_on_Depth = \
            self.Width_squeeze(Z_spatial_integration_on_Depth).permute(0, 4, 2, 3, 1)
        Z_spatial_integration_on_Height = \
            self.Depth_squeeze(Depth_axis).permute(0, 4, 1, 3, 2)
        Z_spatial_integration_on_Height = \
            self.Width_squeeze(Z_spatial_integration_on_Height).permute(0, 4, 2, 3, 1)
        Z_spatial_integration_on_Width = \
            self.Depth_squeeze(Depth_axis).permute(0, 3, 1, 2, 4)
        Z_spatial_integration_on_Width = \
            self.Height_squeeze(Z_spatial_integration_on_Width).permute(0, 3, 2, 1, 4)
        Z_spatial_integration = Z_spatial_integration_on_Depth + \
                                Z_spatial_integration_on_Height + \
                                Z_spatial_integration_on_Width
        channel_descriptor = self.conv_module(Z_spatial_integration)
        recalibrated_feature = torch.mul(input_tensor, channel_descriptor)
        feature_mapping = torch.sum(squared_tensor, dim=1, keepdim=True)
        return recalibrated_feature, feature_mapping

\end{codeblock}