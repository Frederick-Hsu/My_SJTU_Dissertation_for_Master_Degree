% !TEX root = ../main.tex

\chapter{实验的部分源代码}\label{chap:reproduce_experiment}

以下为实验中编写的部分源代码，以供复现实验结果或者帮助理解论文中的一些关键的计算过程与算法。

为复现注意力蒸馏后的效果，请查看以下一些代码片段:
\begin{lstlisting}[
    language=python,
    basicstyle=\ttfamily\zihao{6},
    breaklines=true,
    keywordstyle=\bfseries\color{blue},
    emph={self},
    emphstyle={\bfseries\color{red}},
    commentstyle=\itshape\color{gray},
    stringstyle=\ttfamily\color{green!40!black!100},
    columns=fixed,
    numbers=left,
    numberstyle=\zihao{6}\ttfamily,
    frame=lrtb,
    tabsize=4,
    captionpos=t,
    caption={\bf Attention Distillation Effect},
    label={code:ad_effect}
]

import numpy as np
import SimpleITK as sitk
from matplotlib import pyplot as plt

def load_CT_scan_3D_image(niigz_file_name):
    itkimage = sitk.ReadImage(niigz_file_name)
    numpyImages = sitk.GetArrayFromImage(itkimage)
    numpyOrigin = np.array(list(reversed(itkimage.GetOrigin())))
    numpySpacing = np.array(list(reversed(itkimage.GetSpacing())))
    return numpyImages, numpyOrigin, numpySpacing

#================================================================================

ATM_074_0000_niigz_files = ["ATM_074_0000-feat6.nii.gz", 
                            "ATM_074_0000-feat7.nii.gz", 
                            "ATM_074_0000-feat8.nii.gz", 
                            "ATM_074_0000-feat9.nii.gz"]

feat6_image_npy, feat6_origin, feat6_spacing = 
	load_CT_scan_3D_image(ATM_074_0000_niigz_files[0])

feat7_image_npy, feat7_origin, feat7_spacing = 
	load_CT_scan_3D_image(ATM_074_0000_niigz_files[1])

feat8_image_npy, feat8_origin, feat8_spacing = 
	load_CT_scan_3D_image(ATM_074_0000_niigz_files[2])

feat9_image_npy, feat9_origin, feat9_spacing = 
	load_CT_scan_3D_image(ATM_074_0000_niigz_files[3])
	

depth, height, width = feat6_image_npy.shape

plt.figure(figsize=(10, 10))
plt.imshow(np.flipud(feat6_image_npy[:, height//2, :]))
plt.imshow(np.flipud(feat6_image_npy[:, height//2, :]), cmap="gray")

plt.figure(figsize=(10, 10))
plt.imshow(np.flipud(feat7_image_npy[:, height//2, :]))
plt.imshow(np.flipud(feat7_image_npy[:, height//2, :]), cmap="gray")

plt.figure(figsize=(10, 10))
plt.imshow(np.flipud(feat8_image_npy[:, height//2, :]))
plt.imshow(np.flipud(feat8_image_npy[:, height//2, :]), cmap="gray")

plt.figure(figsize=(10, 10))
plt.imshow(np.flipud(feat9_image_npy[:, height//2, :]))
plt.imshow(np.flipud(feat9_image_npy[:, height//2, :]), cmap="gray")

\end{lstlisting}


通道级特征再学习方法的实现：
\begin{lstlisting}[
    language=python,
    basicstyle=\ttfamily\zihao{6},
    breaklines=true,
    keywordstyle=\bfseries\color{blue},
    emph={self},
    emphstyle={\bfseries\color{red}},
    commentstyle=\itshape\color{gray},
    stringstyle=\ttfamily\color{green!40!black!100},
    columns=fixed,
    numbers=left,
    numberstyle=\zihao{6}\ttfamily,
    frame=lrtb,
    tabsize=4,
    captionpos=t,
    caption={\bf Channel-wise Feature Re-Learning Module},
    label={code:feature_relearning_module}
]

import torch.nn as nn

class FeatureReLearningModule(nn.Module):
    def __init__(self, num_channels, Depth, Height, Width, reduction_ratio=2):
        super().__init__()
        num_reduced_channels = num_channels // reduction_ratio
        self.reduction_ratio = reduction_ratio
        self.conv_module = nn.Sequential(
            nn.Conv3d(in_channels=num_channels, 
                      out_channels=num_reduced_channels, 
                      kernel_size=1, 
                      stride=1),
            nn.ReLU(inplace=True),
            nn.Conv3d(in_channels=num_reduced_channels, 
                      out_channels=num_channels, 
                      kernel_size=1, 
                      stride=1),
            nn.Sigmoid())
        self.spatial_dimension = [Depth, Height, Width]
        self.Depth_squeeze  = nn.Conv3d(in_channels=Depth,  
                                        out_channels=1, 
                                        kernel_size=1, 
                                        stride=1)
        self.Height_squeeze = nn.Conv3d(in_channels=Height, 
                                        out_channels=1, 
                                        kernel_size=1, 
                                        stride=1)
        self.Width_squeeze  = nn.Conv3d(in_channels=Width,  
                                        out_channels=1, 
                                        kernel_size=1, 
                                        stride=1)
    def forward(self, input_tensor):
        squared_tensor = torch.pow(input_tensor, exponent=2)
        # Weight along channels and different axes
        Depth, Height, Width = self.spatial_dimension[0], \
                               self.spatial_dimension[1], \
                               self.spatial_dimension[2]
        Depth_axis = input_tensor.permute(0, 2, 1, 3, 4)        # B, D, C, H, W
        Height_axis = input_tensor.permute(0, 3, 2, 1, 4)       # B, H, D, C, W
        Z_spatial_integration_on_Depth = \
            self.Height_squeeze(Height_axis).permute(0, 4, 2, 1, 3)
        Z_spatial_integration_on_Depth = \
            self.Width_squeeze(Z_spatial_integration_on_Depth).permute(0, 4, 2, 3, 1)
        Z_spatial_integration_on_Height = \
            self.Depth_squeeze(Depth_axis).permute(0, 4, 1, 3, 2)
        Z_spatial_integration_on_Height = \
            self.Width_squeeze(Z_spatial_integration_on_Height).permute(0, 4, 2, 3, 1)
        Z_spatial_integration_on_Width = \
            self.Depth_squeeze(Depth_axis).permute(0, 3, 1, 2, 4)
        Z_spatial_integration_on_Width = \
            self.Height_squeeze(Z_spatial_integration_on_Width).permute(0, 3, 2, 1, 4)
        Z_spatial_integration = Z_spatial_integration_on_Depth + \
                                Z_spatial_integration_on_Height + \
                                Z_spatial_integration_on_Width
        channel_descriptor = self.conv_module(Z_spatial_integration)
        recalibrated_feature = torch.mul(input_tensor, channel_descriptor)
        feature_mapping = torch.sum(squared_tensor, dim=1, keepdim=True)
        return recalibrated_feature, feature_mapping

\end{lstlisting}

在AI超算上申请作业的脚本代码：

\begin{lstlisting}[
    language=bash,
    basicstyle=\ttfamily\zihao{6},
    breaklines=true,
    keywordstyle=\bfseries\color{blue},
    morekeywords={module, load, python3, which, activate},
    emph={self},
    emphstyle={\bfseries\color{red}},
    commentstyle=\itshape\color{gray},
    stringstyle=\ttfamily\color{green!40!black!100},
    columns=fixed,
    numbers=left,
    numberstyle=\zihao{6}\ttfamily,
    frame=lrtb,
    tabsize=4,
    captionpos=t,
    % extendedchars=false,
    caption={\bf Airway3DSegment\_Baseline.slurm},
    label={code:slurm_job_script}
]

#!/bin/bash

#SBATCH --job-name=UNet3D_Airway3DSegment
#SBATCH --partition=dgx2
#SBATCH -n 8
#SBATCH --gres=gpu:2
#SBATCH --output=Airway3DSegment_Baseline_%j.out
#SBATCH --error=Airway3DSegment_Baseline_%j.err
#SBATCH --mail-type=end
#SBATCH --mail-user=frederique.hsu@outlook.com

module load miniconda3

source activate ~/.conda/envs/my-dissertation-env

which python3

python3 --version

# Training the baseline UNet3D model
python3 Airway3DSegmentation.py --model=baseline --batch-size=4 --num-workers=3 \
--save-dir=baseline --train-cube-size 80 192 304 --train-stride 64 96 152 \ 
--val-cube-size 80 192 304 --val-stride 64 72 72 --start-epoch=1 --epochs=60 \
--sgd=0 --randsel=0 --multi-gpu-parallel=1 --encoder-path-ad=0 --decoder-path-ad=0 \
--enable-training=1 --enable-validating=0 --enable-testing=0


# Validating
python3 Airway3DSegmentation.py --model=baseline --batch-size=4 --num-workers=3 \
--save-dir=baseline --train-cube-size 80 192 304 --train-stride 64 96 152 \
--val-cube-size 80 192 304 --val-stride 64 72 72 --start-epoch=1 --epochs=60 \
--sgd=0 --randsel=0 --multi-gpu-parallel=1 --encoder-path-ad=0 --decoder-path-ad=0 \
--resume="./results/baseline/model_latest.ckpt" --save-feature=1 \
--enable-training=0 --enable-validating=1 --enable-testing=0


# Testing
python3 Airway3DSegmentation.py --model=baseline --batch-size=4 --num-workers=3 \
--save-dir=baseline --train-cube-size 80 192 304 --train-stride 64 96 152 \
--val-cube-size 80 192 304 --val-stride 64 72 72 --start-epoch=1 --epochs=60 \
--sgd=0 --randsel=0 --multi-gpu-parallel=1 --encoder-path-ad=0 --decoder-path-ad=0 \
--resume="./results/baseline/model_latest.ckpt" --save-feature=1 \
--enable-training=0 --enable-validating=0 --enable-testing=1

\end{lstlisting}

本论文的全部代码开源在GitHub \href{https://github.com/Frederick-Hsu/Airway3DSegmentNet}{Airway3DSegmentNet}
仓库，若有需要请访问该源代码仓库。